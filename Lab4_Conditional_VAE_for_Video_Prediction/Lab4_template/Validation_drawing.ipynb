{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from modules import Generator, Gaussian_Predictor, Decoder_Fusion, Label_Encoder, RGB_Encoder\n",
    "\n",
    "from dataloader import Dataset_Dance\n",
    "from torchvision.utils import save_image\n",
    "import random\n",
    "import torch.optim as optim\n",
    "from torch import stack\n",
    "\n",
    "from tqdm import tqdm\n",
    "import imageio\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from math import log10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--batch_size BATCH_SIZE] [--lr LR]\n",
      "                             [--device {cuda,cpu}] [--optim {Adam,AdamW}]\n",
      "                             [--gpu GPU] [--test] [--store_visualization] --DR\n",
      "                             DR --save_root SAVE_ROOT\n",
      "                             [--num_workers NUM_WORKERS]\n",
      "                             [--num_epoch NUM_EPOCH] [--per_save PER_SAVE]\n",
      "                             [--partial PARTIAL] [--train_vi_len TRAIN_VI_LEN]\n",
      "                             [--val_vi_len VAL_VI_LEN] [--frame_H FRAME_H]\n",
      "                             [--frame_W FRAME_W] [--F_dim F_DIM]\n",
      "                             [--L_dim L_DIM] [--N_dim N_DIM]\n",
      "                             [--D_out_dim D_OUT_DIM] [--tfr TFR]\n",
      "                             [--tfr_sde TFR_SDE] [--tfr_d_step TFR_D_STEP]\n",
      "                             [--ckpt_path CKPT_PATH] [--fast_train]\n",
      "                             [--fast_partial FAST_PARTIAL]\n",
      "                             [--fast_train_epoch FAST_TRAIN_EPOCH]\n",
      "                             [--kl_anneal_type KL_ANNEAL_TYPE]\n",
      "                             [--kl_anneal_cycle KL_ANNEAL_CYCLE]\n",
      "                             [--kl_anneal_ratio KL_ANNEAL_RATIO]\n",
      "ipykernel_launcher.py: error: ambiguous option: --f=/mnt/left/home/2023/angelina/.local/share/jupyter/runtime/kernel-v2-1194854Cs2WsSev86d4.json could match --frame_H, --frame_W, --fast_train, --fast_partial, --fast_train_epoch\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/DLP_Lab4/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3513: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# evaluate the quality of a figure\n",
    "def Generate_PSNR(imgs1, imgs2, data_range=1.): \n",
    "    \"\"\"PSNR for torch tensor\"\"\"\n",
    "    mse = nn.functional.mse_loss(imgs1, imgs2) # wrong computation for batch size > 1\n",
    "    psnr = 20 * log10(data_range) - 10 * torch.log10(mse)\n",
    "    return psnr\n",
    "\n",
    "\n",
    "class VAE_Model(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(VAE_Model, self).__init__()\n",
    "        self.args = args\n",
    "        \n",
    "        # Modules to transform image from RGB-domain to feature-domain\n",
    "        self.frame_transformation = RGB_Encoder(3, args.F_dim)\n",
    "        self.label_transformation = Label_Encoder(3, args.L_dim)\n",
    "        \n",
    "        # Conduct Posterior prediction in Encoder\n",
    "        self.Gaussian_Predictor   = Gaussian_Predictor(args.F_dim + args.L_dim, args.N_dim)\n",
    "        self.Decoder_Fusion       = Decoder_Fusion(args.F_dim + args.L_dim + args.N_dim, args.D_out_dim)\n",
    "        \n",
    "        # Generative model\n",
    "        self.Generator            = Generator(input_nc=args.D_out_dim, output_nc=3)\n",
    "        \n",
    "        self.optim      = optim.Adam(self.parameters(), lr=self.args.lr)\n",
    "        self.scheduler  = optim.lr_scheduler.MultiStepLR(self.optim, milestones=[2, 5], gamma=0.1)\n",
    "        self.mse_criterion = nn.MSELoss()\n",
    "        self.current_epoch = 0\n",
    "        \n",
    "        # Teacher forcing arguments\n",
    "        self.tfr = args.tfr\n",
    "        self.tfr_d_step = args.tfr_d_step\n",
    "        self.tfr_sde = args.tfr_sde\n",
    "        \n",
    "        self.train_vi_len = args.train_vi_len\n",
    "        self.val_vi_len   = args.val_vi_len\n",
    "        self.batch_size = args.batch_size\n",
    "        \n",
    "        \n",
    "    def forward(self, img, label):\n",
    "        pass\n",
    "    \n",
    "            \n",
    "    @torch.no_grad()\n",
    "    def eval(self):\n",
    "        avg_psnr =[]\n",
    "        val_loader = self.val_dataloader()\n",
    "        for (img, label) in (pbar := tqdm(val_loader, ncols=120)):\n",
    "            img = img.to(self.args.device)\n",
    "            label = label.to(self.args.device)\n",
    "            loss, psnr = self.val_one_step(img, label)\n",
    "            show_dic = {'loss': float(loss.detach().cpu()), 'psnr': psnr}\n",
    "            self.tqdm_bar('val', pbar, show_dic, lr=self.scheduler.get_last_lr()[0])\n",
    "            avg_psnr.append(psnr)\n",
    "        self.psnr = avg_psnr\n",
    "    \n",
    "    \n",
    "    def val_one_step(self, img, label):\n",
    "        # TODO\n",
    "        img = img.permute(1, 0, 2, 3, 4)\n",
    "        label = label.permute(1, 0, 2, 3, 4)\n",
    "\n",
    "        loss, avg_psnr = 0.0, 0.0\n",
    "        pre_out = img[0]\n",
    "        for i in range(1, self.val_vi_len):\n",
    "            # Decoder\n",
    "            z = torch.cuda.FloatTensor(1, self.args.N_dim, self.args.frame_H, self.args.frame_W).normal_()\n",
    "            out_feat = self.frame_transformation(pre_out)\n",
    "            label_feat = self.label_transformation(label[i])\n",
    "            parm = self.Decoder_Fusion(out_feat, label_feat, z)\n",
    "            out = self.Generator(parm)\n",
    "            MSE = self.mse_criterion(out, img[i])\n",
    "\n",
    "            psnr = Generate_PSNR(out, img[i]).item()\n",
    "            avg_psnr += psnr\n",
    "            # if self.args.store_visualization and self.current_epoch == (self.args.num_epoch-1):\n",
    "            #     self.save_history('psnr', psnr)\n",
    "\n",
    "            pre_out = out\n",
    "            loss += MSE\n",
    "            \n",
    "        loss = loss/(self.val_vi_len - 1)\n",
    "        avg_psnr = avg_psnr/(self.val_vi_len - 1)\n",
    "        # self.save_history('avg_psnr', avg_psnr)\n",
    "\n",
    "        return loss, avg_psnr\n",
    "                \n",
    "    def val_dataloader(self):\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((self.args.frame_H, self.args.frame_W)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        dataset = Dataset_Dance(root=self.args.DR, transform=transform, mode='val', video_len=self.val_vi_len, partial=1.0)  \n",
    "        val_loader = DataLoader(dataset,\n",
    "                                  batch_size=1,\n",
    "                                  num_workers=self.args.num_workers,\n",
    "                                  drop_last=True,\n",
    "                                  shuffle=False)  \n",
    "        return val_loader\n",
    "    \n",
    "\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        \n",
    "        checkpoint = torch.load(os.path.join('/mnt/left/home/2023/angelina/DLCourse/Lab4/Result_Mono_0427/epoch=80.ckpt'))\n",
    "        self.load_state_dict(checkpoint['state_dict'], strict=True) \n",
    "        self.args.lr = checkpoint['lr']\n",
    "        self.tfr = checkpoint['tfr']\n",
    "        \n",
    "        self.optim      = optim.Adam(self.parameters(), lr=self.args.lr)\n",
    "        self.scheduler  = optim.lr_scheduler.MultiStepLR(self.optim, milestones=[2, 4], gamma=0.1)\n",
    "        self.current_epoch = checkpoint['last_epoch']\n",
    "\n",
    "    def optimizer_step(self):\n",
    "        nn.utils.clip_grad_norm_(self.parameters(), 1.)\n",
    "        self.optim.step()\n",
    "\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    \n",
    "    os.makedirs(args.save_root, exist_ok=True)\n",
    "    model = VAE_Model(args).to(args.device)\n",
    "    model.load_checkpoint()\n",
    "    model.eval()\n",
    "\n",
    "parser = argparse.ArgumentParser(add_help=True)\n",
    "parser.add_argument('--batch_size',    type=int,    default=2)\n",
    "parser.add_argument('--lr',            type=float,  default=0.001,     help=\"initial learning rate\")\n",
    "parser.add_argument('--device',        type=str, choices=[\"cuda\", \"cpu\"], default=\"cuda\")\n",
    "parser.add_argument('--optim',         type=str, choices=[\"Adam\", \"AdamW\"], default=\"Adam\")\n",
    "parser.add_argument('--gpu',           type=int, default=1)\n",
    "parser.add_argument('--test',          action='store_true')\n",
    "parser.add_argument('--store_visualization',      action='store_true', help=\"If you want to see the result while training\")\n",
    "parser.add_argument('--DR',            type=str, required=True,  help=\"Your Dataset Path\")\n",
    "parser.add_argument('--save_root',     type=str, required=True,  help=\"The path to save your data\")\n",
    "parser.add_argument('--num_workers',   type=int, default=4)\n",
    "parser.add_argument('--num_epoch',     type=int, default=70,     help=\"number of total epoch\")\n",
    "parser.add_argument('--per_save',      type=int, default=3,      help=\"Save checkpoint every seted epoch\")\n",
    "parser.add_argument('--partial',       type=float, default=1.0,  help=\"Part of the training dataset to be trained\")\n",
    "parser.add_argument('--train_vi_len',  type=int, default=16,     help=\"Training video length\")\n",
    "parser.add_argument('--val_vi_len',    type=int, default=630,    help=\"valdation video length\")\n",
    "parser.add_argument('--frame_H',       type=int, default=32,     help=\"Height input image to be resize\")\n",
    "parser.add_argument('--frame_W',       type=int, default=64,     help=\"Width input image to be resize\")\n",
    "\n",
    "\n",
    "# Module parameters setting\n",
    "parser.add_argument('--F_dim',         type=int, default=128,    help=\"Dimension of feature human frame\")\n",
    "parser.add_argument('--L_dim',         type=int, default=32,     help=\"Dimension of feature label frame\")\n",
    "parser.add_argument('--N_dim',         type=int, default=12,     help=\"Dimension of the Noise\")\n",
    "parser.add_argument('--D_out_dim',     type=int, default=192,    help=\"Dimension of the output in Decoder_Fusion\")\n",
    "\n",
    "# Teacher Forcing strategy\n",
    "parser.add_argument('--tfr',           type=float, default=1.0,  help=\"The initial teacher forcing ratio\")\n",
    "parser.add_argument('--tfr_sde',       type=int,   default=10,   help=\"The epoch that teacher forcing ratio start to decay\")\n",
    "parser.add_argument('--tfr_d_step',    type=float, default=0.1,  help=\"Decay step that teacher forcing ratio adopted\")\n",
    "parser.add_argument('--ckpt_path',     type=str,    default=None,help=\"The path of your checkpoints\")   \n",
    "\n",
    "# Training Strategy\n",
    "parser.add_argument('--fast_train',         action='store_true')\n",
    "parser.add_argument('--fast_partial',       type=float, default=0.4,    help=\"Use part of the training data to fasten the convergence\")\n",
    "parser.add_argument('--fast_train_epoch',   type=int, default=5,        help=\"Number of epoch to use fast train mode\")\n",
    "\n",
    "# Kl annealing stratedy arguments\n",
    "parser.add_argument('--kl_anneal_type',     type=str, default='Cyclical',       help=\"\")\n",
    "parser.add_argument('--kl_anneal_cycle',    type=int, default=10,               help=\"\")\n",
    "parser.add_argument('--kl_anneal_ratio',    type=float, default=1,              help=\"\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "main(args)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLP_Lab4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
